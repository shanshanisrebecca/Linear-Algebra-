
What's a system of linear equations （线性方程组） and some of its representations ?

One of them is as  a set of lines in the  plane(平面上的一组直线 ).Another one is as an array of numbers called a matrix(叫做矩阵的数字数组)

What are singular or non=singular systems?（奇异/非奇异系统）

# 1.A system of linear equations

## Preface

A common machine learning approach to modeling systems is called linear regression.Linear regression is a **supervised machine learning approach**, which means we already collect data on many inputs and an output  and the goal is to cover the relationship between them

When the features of input is larger than one and if we graph this equation, it no longer form a line. Instead it would be graphed as  a plane in three dimensional space.

What if you want to consider more features ?

You simply ass a new weight for each feature, even as the equation of this model gets longer, conceptually it works the same.

By finding the right values for the weight and the biased terms, it should be possible to make accurate predictions of the output or target under the assumption that this is a linear relationship.
$$
\begin{align}
w_1 x_1^{(1)} + w_2 x_2^{(1)} + \cdots + w_n x_n^{(1)} + b &= y^{(1)} \\
w_1 x_1^{(2)} + w_2 x_2^{(2)} + \cdots + w_n x_n^{(2)} + b &= y^{(2)} \\
w_1 x_1^{(3)} + w_2 x_2^{(3)} + \cdots + w_n x_n^{(3)} + b &= y^{(3)}\\
.\\
.\\
.\\
w_1 x_1^{(m)} + w_2 x_2^{(m)} + \cdots + w_n x_n^{(m)} + b &= y^{(m)}
\end{align}
$$


This is  ==a System of Linear Equations==

 >[!key] 
 >What we're saying here with a linear model is that there exits some set of values $w_1 w_2 ...w_n b$ that when multiplied by any of these rows of features and added up like this will be able to provide you with an estimate of your target y for that row.
 

We have a vector of weights called $w$ that is made up of $w_1...w_n$, multiplied by each row of features x in my matrix of features and then add a biased term and set that all equal to $y$, which is a vector of my target variable.

>[!success] Key
>When you're using linear regression as a machine learning model, you're representing the system you're interested in as a system of linear equations.
>In fact, if there were a set of w and b values that allowed you to perfectly predict y given a set of x features then this would be a system of equations you could solve analytically without any machine learning.

# Week 1 
## Systems of Linear Equations
## Representing systems as vaectors and matrices 
## Computing the determinant of matrix

# Overview
$$
\begin{align}
a+c-p=6\\
a-c+2p=4\\
4a-2c+1p=10
\end{align}
$$

- What're the weights, w?  
	- a,c,p
- What are the features, x?
	- The features are the numbers next to the weights.
- The targets, y?
	- 6,4,10
- Is this system singular or non-singular?
	- Do these equations either contradict one another or is there redundant information here?
- Can you solve this system of equations?
- Can you represent this system as a matrix and a vector? 
- Can you calculate the determinant of that matrix?


# 1.1 System of sentences

- System1:
	- The dog is black
	- The cat is orange
	- It contains two sentences and two information.This means the system contains as many pieces of information as sentences and that's called a complete system!
- System 2:
	- The dog is black
	- The dog is black
	- Redundant
- System 3:
	- The dog is black
	- The dog is white
	- Contradict each other->Contradictory system


==When a system is redundant or contradictory ,it's called a singular system.And when a system is complete, it's called a non singular system==

>[!success]
>A non singular system is a system that carries as many pieces of information as sentences.It's the most informative system.
>

Systems of sentences can carry more than two sentences. In fact they can carry as many as we want.

## 1.2 Linear Equations & A system of Linear Equations

- Redundant system: Infinite solutions
- Contradictory system: No solutions
-  Complete system: Unique solution ==Non-singular==

>[!question] What is a linear equation?
> It can contain as many variables as we want but there's a special rule that must be followed in a linear equation :
> We only allow the variables to have numbers or scalars attached to them.


## 1.3

Linear equations can easily be visualized as lines in the coordinate plane.If you have three variables they are planes in space.

Since linear equations can be represented as lines, then systems of linear equations can be represented as arrangements of lines in the plane.
![[Pasted image 20250917234201.png]]
 ![[Pasted image 20250917234913.png]]==Non-singular+Unique solution  ==
![[Pasted image 20250917235002.png]]==Singular+Infinite solutions==

## 1.4

When we turn these constants to zero:
![[Pasted image 20250917235410.png]]

系统三从contradictory变成redundant，但依然是singular

>[!success]
>The constants in the system don't matter when it comes to determining if the system is singular or non-singular.

## 1.5 Singular VS  Non-Singular Matrix

A MATRIX IS SIMPLY AN ARRAY of numbers inside a rectangle 

## 1.6 Linear dependence and independence

Recall that a system of sentences is singular if the second sentence carries the same information as the first one.So a system of equations is singular if the second sentence carries the same information as the first one.This is the concept of linear dependence.

==No relations between the rows->Rows are linear independent-> Non-singular matrix==

## 1.7 Determinant->Faster way to tell singular or non-singular

*Consider a singular martix*
$$
\begin{matrix}
a & b \\
c & d
\end{matrix}
$$
we have
$$
\begin{bmatrix} a & b \end{bmatrix} = k \begin{bmatrix} c & d \end{bmatrix}
$$

$$\begin{align}
ak=c\\
bk=d\\
\frac{c}{a}=\frac{d}{b}=k\\
ad = bc\\
ad-bc=0

\end{align}
$$
==The determinant of the matrix== is $$ad-bc$$
>[!success]
>The determinant is zero if the matrix is singular and nonzero if it's non-singular.

When it comes to a larger matrix this is the same. We just need to consider more diagonals.
![[Pasted image 20250918115940.png]]

 *Special Case*
 ![[Pasted image 20250918120246.png]]
Everything below the diagonal is a zero.All the terms in the determinant are going to contain one of these elements below the diagonal except for the one that takes the entire main diagonal.



## 2.1 Solution to non-singular system

The First step in order to go from a system to a solved system is the following:

In the system on the left, the equations both have a and b on them. You'd like to get one in which the a and the b are isolated and they only appear in one equation each.

The first step will be to look at the second equation and try to **eliminate the variable  a from it**.

==How to manipulate equations==
-> Take some linear equations and produce some other ones that are still true based on the original ones.
- Multiply by a constant
	$$
	\begin{align}
	a+b=10\\
	\rightarrow7a+7b=70
	\end{align}
	$$
- Add two equations
	$$
	\begin{align}
	a+b=10\\
	2a+3b=22
	\rightarrow3a+4b=32
	\end{align}
	$$
	
	
## 2.2 Solution to Singular system

### 1. Redundant
Anything you do to remove one will also remove the other one because the system is singular.This is because **the second equation adds no value to the system**

$$
\begin{align}
a+b=10\\
2a+2b=20\\
\end{align}
$$
Solved system:
$$
\begin{align}
a = x\\
b = 10-x\\
\end{align}
$$

*The solved system has one degree of freedom and if you vary this degree of freedom x, you can get many different solutions to. the system.*
### 2.Contradictory

$$
\begin{align}
a+b = 10\\
2a+2b=24
\end{align}
$$

No solutions

## 2.3 Solving system of equations with more variables

Goal:
- The only equation that contains an a is the first one, and there's no a on the second and the third equations.
	- The way to do this is to normalize the first column.-->Divide each row by the coefficient of a 
	- ![[Pasted image 20250918140550.png]]


## 2.4 Matrix row reduction[Gaussian elimination]

From the original matrix, you can get the matrix in the intermediate system by applying some row manipulations. An important feature of this matrix is that it has ones in the main diagonal and zeros underneath the diagonal.This form of matrix is called row echelon form[行阶梯形式]. And finally some more manipulation will get you to the matrix with ones in the diagonal and zeros everywhere.
![[Pasted image 20250918141357.png]]


Singular system:

![[Pasted image 20250918141453.png]]
![[Pasted image 20250918141602.png]]


### Row echelon form

![[Pasted image 20250918141653.png]]



- On the main diagonal, we have a bunch of ones followed by perhaps a bunch of zeros. You could potential have all ones, but you could also have all zeros.
- Below the diagonal, everything is a zero
- To the right of the ones, any number is allowed.
- To the right of the zeros, everything must be zero.

So for a 2x2 matrix there're only 3 cases:
![[Pasted image 20250918142526.png]]


## 2.5 Row operations that preserve singularity

The same manipulation that you use to solve systems of linear equations can be used in matrices. 
These are called row operations in a matrix and a very important property is that they **preserve the singularity of a matrix**

- Row Switching
- Multiply a row by a non-zero scalar
- Add a row to another row


## 2.6 Rank of a matrix

==How much information that matrix or its corresponding system of linear equations is carrying==

## Example: Compressing Images-Reducing rank

Pixelated images are matrices and the rank of matrix is related to the amount of space thst is needed to store the corresponding image.

There's a powerful technique on singular value decomposition[SVD:奇异值分解], which can reduce the rank of a matrix while changing it as little as possible.



>[!success]
>There's a notion of how much information the system carried__Rank



![[Pasted image 20250918144636.png]]
>[!success] Special relationship between the rank of a matrix and its solution space
>The colution space for each of these matrices is the set of solutions when the constants are zeros

![[Pasted image 20250918144923.png]]

## Rank = The number of rows in the matrix-(Dimension of solution space)


==**A matrix is non-singular if and only if it has full rank**==



![[relation.canvas|relation]]


## 2.7 Rank of a matrix:general cases


>[!success]
>Rank = How much information a system brings

>[!question] Any easier way to calculate the rank?
>Yes! It has to do with the row echelon form of the matrix



## 2.8 Row echelon form

![[Pasted image 20250919105436.png]]


The rank of a matrix is the number of ones in the diadonal of row echelon form.The matrix is non-singular if and only the row echelon form has only ones and no zeros in the diagonal.


![[Pasted image 20250919110155.png]]

	
>[!note]
>In general, pivots different than 1 are allowed.
>But for this class, pivots are 1.

  
## 2.9 Reduced Row echelon form

![[Pasted image 20250919111035.png]]

The difference between a reduced row echelon form and a normal echelon form:
- Each pivot is one
- Any number above a pivot is zero
-![[Pasted image 20250919111310.png]]

==Turn anything above a pivot to 0==

## 2.10 The Gaussian Elimination Algorithm

 先构建增广矩阵：
- First task: Use row operations to set your pivot to 1
- Next:Use row operations to set all the values below your pivot to 0
- Repeat the process row by row to simplify the matrix down to the reduced row echelon form.
After you get row echelon form, you can use back substitution to get the reduced row echelon form.

![[Pasted image 20250919112929.png]]
==summary==
![[Pasted image 20250919113036.png]]

## 3 Vector

## 3.1 Vectors & properties

Vectors can be viewed as arrows in the plane or in a higher dimensional space.Two very important components of vectors are  their magnitude(size) and their direction

- Manhanttan distance/taxicab distance 
	- ![[Pasted image 20250919115050.png]]
- 
- ![[Pasted image 20250919115116.png]]
The first one,the L1-norm(范数) is defined the taxicab distance between the origin and the potion(a,b)
The second measure of the size of vector is the L2-norm defined by the helicopter distance between the origin and the potion(a,b) 

By default, when you don't specify which norm to use, we're using the L2-norm.
![[Pasted image 20250919115701.png]]

## 3.2 The dot product



 ==The L2-norm is always the square root of the dot product between the vector and itself==

It's expected that the dot product has a row vector on the left and acolumn vector on the right.In those cases, you might see the transpose used on one of the vectors.

## 3.3Geometric meaning of The dot product

There are some nice relations between the angle and the dot product
- Orthogonal vectors have dot product 0
- ![[Pasted image 20250919134434.png]]

## 3.4 Multiply a matrix by a vector

 >[!success]
 >从点积（向量x向量）拓展到矩阵x向量
 
 ![[Pasted image 20250919135109.png]]
![[Pasted image 20250919135216.png]]

## 3.5 Vectors and Linear Transformations

$$
\begin{bmatrix}
3&1\\
1&2\\
\end{bmatrix}
$$

This transformation will send every point on the plane in the left to a point in the right.
$$
\begin{bmatrix}
3&1\\
1&2\\
\end{bmatrix}

\times

\begin{bmatrix}
0\\
0
\end{bmatrix}
=
\begin{bmatrix}
0\\
0
\end{bmatrix}

$$
$$
\begin{bmatrix}
3&1\\
1&2
\end{bmatrix}
\times
\begin{bmatrix}
1\\
0
\end{bmatrix}
=
\begin{bmatrix}
3\\
1
\end{bmatrix}
$$
$$
\begin{bmatrix}
3&1\\
1&2
\end{bmatrix}
\times
\begin{bmatrix}
0\\
1
\end{bmatrix}
=
\begin{bmatrix}
1\\
2
\end{bmatrix}
$$

$$
\begin{bmatrix}
3&1\\
1&2
\end{bmatrix}
\times
\begin{bmatrix}
1\\
1
\end{bmatrix}
=
\begin{bmatrix}
4\\
3
\end{bmatrix}
$$
![[Pasted image 20250919142140.png]]
>[!success]
>Linear transformation = change of coordinates

![[Pasted image 20250919142317.png]]




### How to find a corresponding matrix for a linear transformation

We have some incognito matrix we know it sends this fundamental square or basis into this one over here. The goal is to find the entries of the matrix.
$$ 
\begin{align}
(0,0) \rightarrow  (0,0)\\
(1,0) \rightarrow  (3,-1)\\ 
(0,1) \rightarrow  (2,3)\\ 
\end{align}

$$

The matrix that send the vector(1,0)and (0,1) to (3,-1)and(2,3) is precisely the one that has columns (3,-1)' and (2,3)'
![[Pasted image 20250920114838.png]]

That's how to turn a linear transformation into a corresponding matrix.

You only need to look at where the two fundamental vectors (1,0) and (0,1) go, and those are your columns of the matrix. 

## 3.6 Matrix multiplication

Matrix multiplication corresponds to combining two linear transformation into a third one.

If we have a matrix
$$
\begin{bmatrix}
3&1\\
1&2
\end{bmatrix}
$$

![[Pasted image 20250920115534.png]]

The fundamental basis on the left gets sent to the basis on the right.

If we have another matrix and we continue doing the same thing.

$$
\begin{bmatrix}
3&1\\
1&2
\end{bmatrix}
$$
![[Pasted image 20250920121649.png]]
![[Pasted image 20250920121847.png]]

The linear transformations act on the vector on the left , so you multiply matrix times vector
这个很重要！乘法的顺序！因为线性变化是对一个矩阵左边的向量起作用

![[Pasted image 20250920122214.png]]

![[Pasted image 20250920122333.png]]

A matrix product is multiplying two matrices but you can also see it as combining two linear transformations into a third one.

==Take rows from the first matrix and columns form the second.==

## 3.7 The identity matrix

Ones in the diagonal and zeros everywhere else.
![[Pasted image 20250920125438.png]]


## 3.8 Matrix inverse

The inverse matrix is precisely that matrix for which the product of the matrices is the identity matrix. In linear teanformation, the inverse matrix is the one that  undoes the job of the original matrix,namely the one that returns the plane to where it was at the begining.

## 3.9 The condition of having an inverse matrix-->NON-singular

## 3.10 Neural networks & matrices

![[Pasted image 20250920131052.png]]

![[Pasted image 20250920131255.png]]

## 4.1
- What do determinant & singularity mean in terms of linear transformation.
- Eigen vectors & eigen values!

Imagine theis line to be the space and project all the points to the line.
![[Pasted image 20250920133038.png]]

The goal of PCA is to intelligently reduce the number of columns, providing the benefits of a more compact data set without losing all the useful information the original data set contains.


## 4.1 Characterize your linear transformation using the determinant
![[Pasted image 20250920133534.png]]



The orange grid covers every single point on the plane.
![[Pasted image 20250921155950.png]]

If the resulting points after multiplying it by a matrix cover the whole plane, then the transformation is non-singular, and vice versa.

The points that are covered in the right are called the image of the transformation

![[Pasted image 20250921161219.png]]

![[Pasted image 20250921161325.png]]

SO there's another way to calculate rank:  ==The dimension of the image of the linear transformation==

## 4.2 Determinants as an area

 ![[Pasted image 20250921184246.png]]
The determinant of a matrix is the area of the image of the fundamental basis formed by the units square on the left.

A parallelogram can have a negative area depending on  what order we take the two basis vectors as follows.
![[Pasted image 20250921184725.png]]


## 4.3 Determinant of a product

$$
\det (AB) = \det A \times \det B
$$

The transformation blow up the area and we can multiply them

## 4.4 Determinant of inverse



$$
\det A^{-1} = \frac{1}{\det A}
$$


## 4.5 Bases

The main property of a basis is that every point in the space can be expressed as a linear combination of elements in the basis.

Anything that comprises two vectors that go in the same direction and they coubld be opposites ,They're not bases.


## 4.6 span


The span of a set of vectors is simply the set of points that can be reached by walking in  the direction of these vectors in any combination.

==A basis is a minimal spanning set==

The lenth of the basis is the space of the dimension of the plane


==Basis: A formal definition==
A basis is a set of vectors that:
- Spans a vector space
- linearly independent


## 4.7 Eigenbasis


Some bases are more useful than others.In particular, there's one basis to rule them all called the eigenbasis.

**The sides of the two parallelograms are parallel to the correspending one in the other basis**

![[Pasted image 20250921191324.png]]

Becase these sites are parallel the what we're doing to the plane is we're stretching by two in this direction(the horizontal direction) and streching by three in this diagonal direction.

So this is a very special basis. It only consists of two stretching.

==That's what we call an eigenbasis==

It's a very special way of looking at linear transformation with respect to a basis that sends a parallelogram to another parallelogram with sides parallel to the original one.



The two vectors in the basis are goona be called eigenvectors and the stretching factor(2.3) are gonna be called eigenvalues.
![[Pasted image 20250921192032.png]]

## 4.8 Eigenvalues and eigenvectors

![[Pasted image 20250921192242.png]]

for the special vector v1, matrix A multiply v1 is the same as multiplying v1 by the scalar lambda1
$$
A v_{1} = \lambda v_{1}
$$
$$
A v_{2} = \lambda v_{2}
$$

v1 and v2 are the matrix A's eigenvectors and lambda1 and lambda2 are the matrix A's eigenvalues.

==Eigenvectors and eigenvalues come in pairs==

![[Pasted image 20250921192610.png]]

## meaning
On the left side of these equations you have a matrix multiplication.
On the right side you have a scalar multiplication

One big difference is that matrix multiplication is a lot more work!

At least along a matrix's eigenvectors, you can turn a large computation into a smaller one

And if you apply what you know about bases, you can actually use this shortcut everywhere.

![[Pasted image 20250921193043.png]]

So you can solve this linear transformation without doing any matrix multiplication.

![[Pasted image 20250921193337.png]]

## 4.9 Calculating Eigenvalues and Eigenvalues

First take a look at the matrix with entries 
$$
\begin{bmatrix}
2&1\\
0&3


\end{bmatrix}
$$
Take a look at how it acts on these points around the square.
The two horizontal vectors get stretched by 2 and the diagonal get stretched by 3.Now for the other points :
![[Pasted image 20250921194507.png]]

Now take a look at another matrix.One that simply stretched the entire plane by a factor of three in any direction!
$$
\begin{bmatrix}
3&0\\
0&3


\end{bmatrix}
$$

![[Pasted image 20250921194624.png]]

These transformations are not the same transformation, but they do coincide in many points.
In other words, they act the exact same way for infinitely many points:All the points in this lin:
![[Pasted image 20250921194841.png]]

In this diagonal the two transformations do the exact same thing.

It's strange cause the transformation should only atch at one point (0,0)

When they match at infinitely many points,something non-singular is happening

If these two transformations match at infinitely many points that means the difference is 0 at infinitely many points.
$$
\begin{bmatrix}
2&1\\
0&3
\end{bmatrix}
\times
\begin{bmatrix}
x\\
y
\end{bmatrix}
=\begin{bmatrix}
3&0\\
0&3
\end{bmatrix}
\times
\begin{bmatrix}
x\\
y
\end{bmatrix}

$$
If you apply the difference of these two matrix to any vector in any of these diagonals, you get (0,0)

$$
（\begin{bmatrix}
2&1\\
0&3
\end{bmatrix}
-
\begin{bmatrix}
3&0\\
0&3
\end{bmatrix}）
\times
\begin{bmatrix}
x\\
y
\end{bmatrix}
=\begin{bmatrix}
0\\
0
\end{bmatrix}
$$

In other words, this matrix times vector(x,y) for infinity metric vectors is (0,0)
$$
\begin{bmatrix}
-1&1\\
0&0
\end{bmatrix}）
\times
\begin{bmatrix}
x\\
y
\end{bmatrix}
=\begin{bmatrix}
0\\
0
\end{bmatrix}
$$
That's the trait of a singular transformation.

Recall that non-singular transformation has a unique solution to the equation matrix times vector equal (0,0) and that's the vector (0,0).
It means the matrix is singular!

另外一个特征向量方向也是一样的
![[Pasted image 20250921220125.png]]

if lambda is an eigenvalue, then the matrix given by a matrix and the transformation given by scaling the plane entirely by a factor of lambda are equal for infinitely many vectors.

That means their difference times a vector is equal to 0,0 for infitiely many vectors 

That's called the characteristic polynomial.

# The way to find eigenvalues:
 Look at the characteristic polynomial and find the roots.
 ![[Pasted image 20250921220712.png]]

$$
\det(A-\lambda I)=0
$$

只有方阵才有行列式 所以只有方阵才有特征值和特征向量。
矩形的没有哦！

## The number of eigenvectors


![[Pasted image 20250921222437.png]]
![[Pasted image 20250921222941.png]]
![[Pasted image 20250921223039.png]]

## 5.1 Dimensionality Reduction and Projection

The original data set has many rows or many columns or features that store useful information about each observation.PCA will reduce the number of features in your table while maintaing the same number of observations.

Put another way , the data set will have the same number of rows and fewer columns.

It will be just as tall but it will get skinnier.

The idea behind dimensionality reduction is to move data points into a vector space with fewer dimensions. This is called a projection. 

Multiply bt the vectir projects the points along that vector and dividing by the vector'snorm ensures that there's no stretching introduced.
![[Pasted image 20250921230129.png]]

![[Pasted image 20250921230322.png]]

Now the question is:
How to pick the vectors to project onto/

## 5.2 Motivating PCA

Benefits of PCA
- Easier dataset to  manage
- PCA
-  reduces dimensions while minimizing information loss
- simpler visualization

## 5.3 Variance and covariance 

Variance describes how spread out your data is.

Covariance helps measure how two features of a dataset varies with respect toon eanother

![[Pasted image 20250922213401.png]]\

==Covariance: measure the  direction of the relationship between two variables==

## 5.4 Covariance Matrix

 ![[Pasted image 20250922215910.png]]
First calculate the variance of each variable and the covariance of each combination of variables

Next build a matrix

![[Pasted image 20250922220032.png]]

 Store all the points in a matrix. Each row will be an observation of X and Y
Each column contains all the observations for a single variable

We need a matrix that has the same shape as A and each column takes on the mean value for the variable called Mu

$$
C = \frac{1}{n-1}(A-\mu)^T(A-\mu)
$$
![[Pasted image 20250922220420.png]]

![[Pasted image 20250922220543.png]]

![[Pasted image 20250922220802.png]]

## 5.5 PCA Overview

Best line = Most variance

Remember eigenvector gives a direction and eigenvalue gives a number

Every covariance matrix is symmetric so the eigenvectors you calculate are always orthogonal


eigenvectors = PCA components

The eigenvector with the largest eigenvalue will always be the one that will give the greatest variance when you project your data

![[Pasted image 20250922221730.png]]

## Principle of PCA

Consider the covariance matrix as a linear transformation 
so (1,0) goes to (9,4) and (0,1) goes to (4,3)
And Consider a circle with radius 1:
![[Pasted image 20250922222324.png]]

Two eigenvectors (2,1)(-1,2)form a basis

The goal of getting eigenvalues and eigenvectors is to reframe a linear transformation as teo stretches.
Any point along the vector (2,1) will be stretched by a factor of 11
Any point along the vector (-1,2) will be stretched by a factor of 1
Any other vector in the plane would be stretched by a factor somewhere between 1 and 11



For example,(1,0) goes to (9,4),stretched by 9.85

==Covariance matrix C characterizes the spread out of the data.The matrix C's eigenvectors tell you the direction in which the matrix can be viewed as just straight stretching.The largest eigenvalue tells you where that stretching is the largest and any other direction will be stretched out less.So choosing the eigenvectors with the biggest eigenvalues will givw the direaction with the biggest stretch or biggest variance==

## 5.7 PCA Mathematical Formular

![[Pasted image 20250922223512.png]]


All values are positive and each colum sum up to1-> Markov matrix
They allow you to infer the probability of how your system will evolve.

If today is cloudy, you can represent that by (0,1,0)‘, which is called the state vector

![[Pasted image 20250922230148.png]]
![[Pasted image 20250922230214.png]]

到最后迭代会得到一个他的特征向量，并且特征值是1，所以stable

This matrix is called transition matrix(should be a markov with each column made up of non-negative values that add up to 1 first)

This vector is called the equilibrium vector and it gives you the long-run probability that on a given day it will be sunny,cloudy or rainy.
==No matter waht your initial state was, in the limit at infinity, you will get to this equilibrium state==


